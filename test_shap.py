import pandas as pd
import numpy as np
import os

import sys

import warnings
warnings.filterwarnings('ignore')


## Importamos funciones para el cálculo de los errores
from sklearn.metrics import mean_squared_error, mean_absolute_error, root_mean_squared_error

## Load Machine Learning Models
from xgboost import XGBRegressor

from sklearn.model_selection import RandomizedSearchCV
import matplotlib.pyplot as plt

### Define rutas
def build_path(PATH : list) -> str:
    return os.path.abspath(os.path.join(*PATH))

FILE_PATH = os.getcwd()

DATA_PATH = build_path([FILE_PATH, "data"])
TS_FILE_PATH = build_path([DATA_PATH, "dataset_clean_slv_administrativos_reales.xlsx"])


### Carga datos
df_slv = pd.read_excel(TS_FILE_PATH, sheet_name="DATOS").iloc[60:]
df_slv = df_slv[~df_slv.duplicated(subset=["anio_trim"])].reset_index(drop=True)
descriptor = pd.read_excel(TS_FILE_PATH, sheet_name="DICCIONARIO")

### Características
#COMPLETOS = True
datos_dict = {"completos" : True, "parciales" : False}

COMPLETOS = datos_dict["completos"]

if COMPLETOS:
    features = ["viirs_bm_sum", "viirs_bm_mean", "viirs_csm_sum", "viirs_csm_mean",  "ndvi_gee", "ndvi_wfp", "evi_gee", "ndbi_gee","temp_air", "temp_ls", "precip", "disaster_occurrence", "disaster_count", "flood_count", "mass_movement_(wet)_count", "drought_count", "storm_count","earthquake_count","volcanic_activity_count", "extreme_temperature_count", 'exports_usd_fob', 'imports_usd_cif', 'remesas_usd_trim',
        #'remesas_usd_trim_g', 'consumo_elect_al_publico',
        'consumo_elect_al_publico',
        'consumo_elect_comercio', 'consumo_elect_especiales',
        'consumo_elect_industria', 'consumo_elect_residencial',
        'consumo_elect_total', 'gdp_us_const_trim']
else:
    features = ["viirs_bm_sum", "viirs_bm_mean", "viirs_csm_sum", "viirs_csm_mean", "ndvi_gee", "ndvi_wfp", "evi_gee", "ndbi_gee","temp_air", "temp_ls", "precip", "disaster_occurrence", "disaster_count", "flood_count", "mass_movement_(wet)_count", "drought_count", "storm_count","earthquake_count","volcanic_activity_count", "extreme_temperature_count"]

### Formato fecha
date_range = pd.date_range(start='1/1/2005', periods=79, freq='QE')
df_slv["Date"] = date_range
df_slv.set_index('Date', inplace=True)

### Diccionario de variables de respuesta
respuesta_vars = {
    "nominal" : "pib_bc_usd", # PIB trimestral en USD
    "real" : "pib_bc_usd_real",# PIB trimestral en USD reales
}

pib_respuesta = respuesta_vars["nominal"]

### Aplicamos transformación logarítmica a la variable de respuesta
df_slv["log_pib_bc_usd"] = np.log(df_slv[pib_respuesta])

### Calculamos la primer diferencia del logaritmo de la variable de repuesta
df_slv["pib_bc_usd_diff"] = [np.nan] + list(np.diff(df_slv["log_pib_bc_usd"], n=1))

### Calculamos la tasa de crecimiento entre trimestres de la variable de respuesta
df_slv["tc_pib_bc_usd"] = df_slv[pib_respuesta].pct_change()


y_targets_name = ["log_pib_bc_usd", "pib_bc_usd_diff", "tc_pib_bc_usd"]

#y_colname_target = "DIFF LOG PIB"
y_colname_target = "TC-PIB"

# Define features and target
y_map_column_target = {
    "DIFF-LOG-PIB" : "pib_bc_usd_diff",
    "LOG-PIB" : "log_pib_bc_usd",
    "TC-PIB" : "tc_pib_bc_usd",
}


### Remueve de los datos de entrenamiento variables del resto del PIB
excluye_pib = [j for i,j in y_map_column_target.items() if i!=y_colname_target]
df_slv = df_slv[[i for i in df_slv.columns if not i in excluye_pib]]

# Create lag features for the dataset
lagged_data = df_slv[[y_map_column_target[y_colname_target]]+features].copy()

# Function to create lag features
def create_lagged_features(data, feature_name , lags=3):
    df = data.copy()
    for lag in range(1, lags + 1):
        df[f'lag_{lag}_{feature_name}'] = df[feature_name].shift(lag)
    #df.dropna(subset=[f'lag_{lag}_{feature_name}' for lag in range(1, lags + 1) ], inplace=True)  # Drop rows with NaN values generated by lagging
    return df

for i in [y_map_column_target[y_colname_target]] + features:
    lagged_data = create_lagged_features(lagged_data, i, lags=4)

# Function to add rolling features
def add_rolling_features_mean(df, feature_name, window=4):
    return df[feature_name].rolling(window=window).mean().shift(1)

def add_rolling_features_std(df, feature_name, window=4):
    return df[feature_name].rolling(window=window).std().shift(1)


for i in [y_map_column_target[y_colname_target]]+features:
    lagged_data[f"{i}_mean"] = add_rolling_features_mean(lagged_data, i, window=3)
    lagged_data[f"{i}_std"] = add_rolling_features_std(lagged_data, i, window=3)

## Definimos función que calcula los errores dado un modelo

def compute_errors(model, model_name):
    ypred_trn = model.predict(X_train)
    ypred_tst = model.predict(X_test)

    mse_train, mse_test = mean_squared_error(y_train, ypred_trn), mean_squared_error(y_test, ypred_tst)
    mae_train, mae_test = mean_absolute_error(y_train, ypred_trn), mean_absolute_error(y_test, ypred_tst)
    rmse_train, rmse_test = root_mean_squared_error(y_train, ypred_trn), mean_absolute_error(y_test, ypred_tst)

    print('Train MSE = {:.6f}, Test MSE = {:.6f}\n' \
          'Train MAE = {:.6f}, Test MAE = {:.6f}\n'\
          'Train RMSE = {:.6f}, Test RMSE = {:.6f}'.format(mse_train, mse_test, mae_train, mae_test, rmse_train, rmse_test))

    data = [(model_name, "Training", mse_train, mae_train, rmse_train),
            (model_name, "Test", mse_test, mae_test, rmse_test)]

    return pd.DataFrame(data, columns=["Modelo", "Datos", "MSE", "MAE", "RMSE"])

# Split the data into training and testing sets (80% train, 20% test)
# Train set : 2012Q1-2018Q4
# Test set : 2019Q1-2024Q3
train_data = lagged_data.loc["2012":"2018"]
test_data = lagged_data.loc["2019":]


## SKLEAR imputaciones
## https://scikit-learn.org/stable/modules/impute.html

from sklearn.impute import KNNImputer
imputer = KNNImputer(n_neighbors=2, weights="uniform")

## Define training and test set

X_features_names = list(lagged_data.columns)
X_features_names.remove(y_map_column_target[y_colname_target])

y_train = train_data[y_map_column_target[y_colname_target]]
y_test = test_data[y_map_column_target[y_colname_target]]

X_train = imputer.fit_transform(train_data[X_features_names])
X_test = imputer.fit_transform(test_data[X_features_names])

print("""
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
TRAINING {}\nTuning the hyper-parameters with RandomizedSearchCV
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n
""".format("XGBOOST"))

parameters = {
              "loss" : ["linear", "square", "exponential"],
              "learning_rate": 2**np.arange(-8., 2., step=2),
              "n_estimators" : [1,200]
            }

xgb = XGBRegressor(objective='reg:pseudohubererror', random_state=0)
param_tuner_xgb = RandomizedSearchCV(xgb, parameters, n_iter=20, cv=5, refit=True, verbose=1)
param_tuner_xgb.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)


errores = compute_errors(param_tuner_xgb.best_estimator_, "XGBoost")


## Cálculo de los errores promedio promedio de los modelos pre-Bukele y post Bukele
def get_fitted_and_errors(model, model_name, y_target_name, periodo_inicio, periodo_final):
    data = lagged_data.loc[periodo_inicio:periodo_final]

    X_data = imputer.fit_transform(data[X_features_names])
    y_data = data[y_target_name]

    y_predicted = model.predict(X_data)

    errores = y_data - y_predicted

    model_errores_raw = pd.DataFrame(zip(y_predicted, errores), columns = [f"{model_name}-Fitted", f"{model_name}-Errors"])

    return  model_errores_raw

fitted_value = get_fitted_and_errors(param_tuner_xgb.best_estimator_, "XGBoost", y_map_column_target[y_colname_target], "2012Q1", "2024Q3")
fitted_value["Tasa Crecimiento PIB Nominal"] = pd.concat([y_train, y_test]).values
fitted_value["date"] = pd.concat([y_train, y_test]).index

fitted_value.set_index("date", inplace = True)

fitted_value.drop(columns = "XGBoost-Errors").plot()
plt.show()

fitted_value.drop(columns = "XGBoost-Errors").iloc[-5:]

# explain the model's predictions using SHAP
# (same syntax works for LightGBM, CatBoost, scikit-learn, transformers, Spark, etc.)
import shap

explainer = shap.Explainer(param_tuner_xgb.best_estimator_, feature_names=X_features_names)
shap_values = explainer(X_test)

# visualize the first prediction's explanation
shap.plots.waterfall(shap_values[-1])

df_shapley_values = pd.DataFrame({"Característica" : X_features_names, "Contribución" : shap_values[-1].values , "Absolutos" : np.abs(shap_values[-1].values)})

df_contribucion = df_shapley_values.sort_values("Absolutos", ascending=False).iloc[:10].drop(columns = ["Absolutos"])
df_contribucion

# visualize the first prediction's explanation with a force plot
shap.plots.force(shap_values[-1])

